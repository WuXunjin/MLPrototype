{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct a dummy experiment where we have three modules (set of linear layers) < A, F and B >. We would like to freeze the weights pertaining to module F. \n",
    "\n",
    "Aim : Check during training that these weights are not altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## custom dataloaders\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Datasets.loadDataset import loadDataset, getChannels, checkAndCreateFolder\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from networks import EmbeddingNet, TripletNet, SiameseNet, LeNet\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleA, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(512, 256)\n",
    "        self.layer2 = nn.Linear(256,128)\n",
    "        \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)       \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x), inplace=True)\n",
    "        x = F.relu(self.layer2(x), inplace=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleF, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(128, 64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)       \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x), inplace=True)\n",
    "        x = F.relu(self.layer2(x), inplace=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleB, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(32, 16)\n",
    "        self.layer2 = nn.Linear(16,1)\n",
    "        \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)       \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x), inplace=True)\n",
    "        x = F.relu(self.layer2(x), inplace=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instances pertaining to the three modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "inputSize = 512\n",
    "learningRate = 0.0002\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we backpropagate the error, we only want to update modles A and B, leaving the weights of module FR as frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the optimizer and loss function. Let us assume that we would like to have an output value of 1 for all the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aInit = 'A_init.pt'\n",
    "fInit = 'F_init.pt'\n",
    "bInit = 'B_init.pt'\n",
    "\n",
    "aFinal = 'A_fin.pt'\n",
    "fFinal = 'F_fin.pt'\n",
    "bFinal = 'B_fin.pt'\n",
    "\n",
    "modelFolder = 'dummy'\n",
    "checkAndCreateFolder(modelFolder)\n",
    "\n",
    "\n",
    "def train(activate_a=True, activate_f=True, activate_b=True):\n",
    "    \n",
    "    # instantiate the three modules of neural network\n",
    "    A = ModuleA()\n",
    "    FR = ModuleF()\n",
    "    B = ModuleB()\n",
    "    \n",
    "    for param in FR.parameters():\n",
    "        param.requires_grad=False\n",
    "    \n",
    "    # initialize with random weights\n",
    "    A.weight_init(0.0,0.02)\n",
    "    FR.weight_init(0.0,0.02)\n",
    "    B.weight_init(0.0,0.02)\n",
    "    \n",
    "    PATH = modelFolder+'/'+aInit\n",
    "    torch.save(A.state_dict(), PATH)\n",
    "    PATH = modelFolder+'/'+fInit\n",
    "    torch.save(FR.state_dict(), PATH)\n",
    "    PATH = modelFolder+'/'+bInit\n",
    "    torch.save(B.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # let's say we wish the combined network to output 1 for every input\n",
    "    idealOutput = torch.ones(batchSize,\n",
    "                             1)\n",
    "    idealOutputV = Variable(idealOutput)\n",
    "\n",
    "    # take the mean square error loss\n",
    "    lossFunction = nn.MSELoss()\n",
    "    \n",
    "    # define optimzier for the three components of our network\n",
    "    AOptimizer = optim.Adam(A.parameters(),\n",
    "                            lr=learningRate,\n",
    "                            betas = (0.5,0.999))\n",
    "    '''\n",
    "    FOptimizer = optim.Adam(FR.parameters(),\n",
    "                            lr=learningRate,\n",
    "                            betas = (0.5,0.999))\n",
    "                            '''\n",
    "    BOptimizer = optim.Adam(B.parameters(),\n",
    "                            lr=learningRate,\n",
    "                            betas = (0.5,0.999))\n",
    "    \n",
    "    for epoch in range(epochs) :\n",
    "        \n",
    "        # at the beginning of each epoch set the gradients to each node in computational graph as 0\n",
    "        A.zero_grad()\n",
    "        FR.zero_grad()\n",
    "        B.zero_grad()\n",
    "\n",
    "        # input is sampled from a unit Gaussian\n",
    "        inputNN = torch.FloatTensor(batchSize,\n",
    "                                    inputSize).random_(0,1)\n",
    "\n",
    "        inputNNV = Variable(inputNN, requires_grad=True)\n",
    "\n",
    "        # require_grad for each node is True as input also requires gradient\n",
    "        outputA = A(inputNNV)\n",
    "        outputF = FR(outputA)\n",
    "        outputB = B(outputF)\n",
    "        \n",
    "        # calculate the loss function\n",
    "        loss = lossFunction(outputB, idealOutputV)\n",
    "\n",
    "        # calculate gradient for each leaf node\n",
    "        loss.backward()\n",
    "        \n",
    "        # update according to the optimization method used\n",
    "        if activate_b:\n",
    "            BOptimizer.step()\n",
    "        \n",
    "\n",
    "        \n",
    "        if activate_a:\n",
    "            AOptimizer.step()\n",
    "            \n",
    "        #print loss\n",
    "        \n",
    "    PATH = modelFolder+'/'+aFinal\n",
    "    torch.save(A.state_dict(), PATH)\n",
    "    PATH = modelFolder+'/'+fFinal\n",
    "    torch.save(FR.state_dict(), PATH)\n",
    "    PATH = modelFolder+'/'+bFinal\n",
    "    torch.save(B.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(activate_a=True, activate_b=True, activate_f=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BInit = ModuleB()\n",
    "PATH = modelFolder + '/' + bInit\n",
    "BInit.load_state_dict(torch.load(PATH))\n",
    "\n",
    "FInit = ModuleF()\n",
    "PATH = modelFolder + '/' + fInit\n",
    "FInit.load_state_dict(torch.load(PATH))\n",
    "\n",
    "AInit = ModuleA()\n",
    "PATH = modelFolder + '/' + aInit\n",
    "AInit.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFin = ModuleB()\n",
    "PATH = modelFolder + '/' + bFinal\n",
    "BFin.load_state_dict(torch.load(PATH))\n",
    "\n",
    "FFin = ModuleF()\n",
    "PATH = modelFolder + '/' + fFinal\n",
    "FFin.load_state_dict(torch.load(PATH))\n",
    "\n",
    "AFin = ModuleA()\n",
    "PATH = modelFolder + '/' + aFinal\n",
    "AFin.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change():\n",
    "    \n",
    "    BL1 = (BFin._modules['layer1'].weight == BInit._modules['layer1'].weight).data.all()\n",
    "    BL2 = (BFin._modules['layer2'].weight == BInit._modules['layer2'].weight).data.all()\n",
    "    \n",
    "    if BL1 and BL2:\n",
    "        print 'Module B : Weights are same'\n",
    "    else :\n",
    "        print 'Modele B : Weights changed'\n",
    "    \n",
    "    AL1 = (AFin._modules['layer1'].weight == AInit._modules['layer1'].weight).data.all()\n",
    "    AL2 = (AFin._modules['layer2'].weight == AInit._modules['layer2'].weight).data.all()\n",
    "    \n",
    "    if AL1 and AL2:\n",
    "        print 'Module A : Weights are same'\n",
    "    else :\n",
    "        print 'Modele A : Weights changed'    \n",
    "        \n",
    "    FL1 = (FFin._modules['layer1'].weight == FInit._modules['layer1'].weight).data.all()\n",
    "    FL2 = (FFin._modules['layer2'].weight == FInit._modules['layer2'].weight).data.all()\n",
    "    \n",
    "    if FL1 and FL2:\n",
    "        print 'Module F : Weights are same'\n",
    "    else :\n",
    "        print 'Modele F : Weights changed'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele B : Weights changed\n",
      "Modele A : Weights changed\n",
      "Module F : Weights are same\n"
     ]
    }
   ],
   "source": [
    "change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear (32 -> 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BInit._modules['layer1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleB' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-52916376e9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBInit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/lovish.chum/pytorch/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 262\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModuleB' object has no attribute 'layer'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
